\section{The Hessian Matrix}
\label{sec:hessian}

For a real function $f$ of $n$ variables, $\vect{x} = (x_1, x_2, \ldots, x_n)$,
there exists an $n\times n$ matrix, $\text{H}$, which contains all the second partial derivatives,
\beq{hessian-matrix}
\text{H} =
\begin{bmatrix}
\vspace{0.5em} % To create a bit of space AFTER the first line.
\frac{\partial^2f}{\partial x_1^2} &
\frac{\partial^2f}{\partial x_1 \partial x_2} &
\cdots &
\frac{\partial^2f}{\partial x_1 \partial x_n} \\

\frac{\partial^2f}{\partial x_2 \partial x_1} &
\frac{\partial^2f}{\partial x_2^2} & 
\cdots &
\frac{\partial^2f}{\partial x_2 \partial x_n} \\

\vdots & \vdots & \ddots & \vdots \\

\frac{\partial^2f}{\partial x_n \partial x_1} &
\frac{\partial^2f}{\partial x_n \partial x_2} &
\cdots &
\frac{\partial^2f}{\partial x_n^2} &
\end{bmatrix}.
\eeq
The second derivative of a function represents, in particular, information about its local curvature, or how rapidly the first derivative changes.
Classification of stationary points (and implicitly critical points) relies on the eigenvalues of $\text{H}$, $\lambda_i$,
\bit
\item Minimum if all $\lambda_i > 0$
\item Maximum if all $\lambda_i < 0$
\item Saddle Point if some but not all $\lambda_i < 0$
\item Otherwise higher order derivatives are needed for classification
\eit

$\text{H}$ is named after the German mathematician Ludwig Otto Hesse and is commonly referred to as the Hessian matrix, or Hessian for short~\cite{hessian}.

In the context of atomic simulations, $n$ is generally 3 times the number of atoms in the system, as each one has 3 independent degrees of freedom and the function in question is often the potential energy of the system.
In most modern software packages both the potential energy and force are readily available while the second order derivatives are, generally, not available without explicit and, often, costly calculations.
