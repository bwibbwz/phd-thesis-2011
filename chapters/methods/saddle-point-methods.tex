\section{First Order Saddle Point Methods}
\label{sec:sp-methods}

Finding saddle points is a non-trivial task in multiple dimensions, when only local information is available and computational resources are limited, such that calculation of the Hessian matrix is infeasible.

---Need a bunch of citations here

\incomplete

% ------------------------------------------------------------------
\subsection{Saddle Points}
\label{sec:sps}

Saddle points are stationary points, i.e. with zero gradient, on multidimensional function, $f(\vR)$, that are neither maxima nor minima.

The most common image of a saddle (point) is the function $f(x, y) = x^2 - y^2$ which near $(x,y) = (0,0)$ resembles a saddle, used when riding horses (se figure...), curving upwards in one direction and downwards in the other.
\figmiss{Comparison of a saddle points environment and a saddle used on a horse.}
This most common image of a saddle point lacks a few elements to be to tell their whole story.

On functions of higher dimensionality than $2$, different orders of saddle points are possible.
The order of the saddle point is decided by the amount of directions that are at a maximum, rather than a minimum.
As such, figure ... show a first order saddle point on a two dimensional function.

\bit
\item What about 1D saddles (e.g. $f(x = 0) = x^3$)
\item What about similar constructs in more dimensions (e.g. like the above in one direction but a maximum in the other)
\eit

\recent

\incomplete

% ------------------------------------------------------------------
\subsection{Nudged Elastic Band}
\label{sec:neb}

Finding Steepest Decent Paths (SDPs) from a given point is simple by following the gradient with a small step size.
On the other hand, finding specific SDPs that end at minima is not.
The goal here is to find two SDPs each leading from the same \sap1 to different minima without any further information than the minima.


\incomplete

% ------------------------------------------------------------------
\subsection{Dimer}
\label{sec:dimer}

The Dimer algorithm~\cite{dimer-original-1999} is in essence a method for finding the eigenmode corresponding to the lowest eigenvalue of the Hessian, while performing no direct calculations of the second derivatives.
This information is then used to locate \sap1s.

%The Dimer method was inspired by Voter\cite{voter-hyperdynamics-1997}

Given only an initial point, $\vR$, on a multidimensional function, $V(\vR)$, the goal is to, iteratively, locate a nearby \sap1, using no direct calculation of the Hessian, i.e. using only the function's values and its gradient, $\nabla V(\vR)$.
Indirect information about the Hessian is, however, used in the form of an estimate of the eigenmode corresponding to its lowest eigenvalue (the minimum mode).
Using the minimum mode, $\uvn$, it is possible to locally transform \sap1s to minima while using conventional techniques to move up-hill and locate the \sap1.

The dimer method can be split into three independent phases.
\ben{dimer-phases}
\item Estimating the minimum mode.
\item Transforming the gradient to make \sap1 seem as minima.
\item Translating the point according to the transformed gradient.
\een
Only the first of the phases is unique to the dimer algorithm.
A setup phase, which typically includes randomly displacing $\vR$ is also required if the search starts from a minimum.
\recent

\subsubsection{Minimum Mode Estimation}
\figmiss{Dimer force diagram, showing all forces. [Already made it!]}
Inspired by the hyperdynamics method by Voter~\cite{voter-hyperdynamics-1997}.

Estimating the second derivative of $V$ along a given unit vector, $\uvs$, at point $\vR$ can be done numerically, using finite differences.
For the occasion, a pair of points (the dimer), $[\vR_\text{A}, \vR_\text{B}]$, are chosen, close to current point $\vR_0$, such that
\beq{dimer-separation}
\vR_\text{A} = \vR_0 + \Dsep \uvs \quad \text{and} \quad \vR_\text{B} = \vR_0  - \Dsep \uvs,
\eeq
where $\Dsep$ is a predefined constant to determine the length of the dimer and the separation in the finite difference estimate.
Using only the function's values, the second derivative (or curvature), $C_\vs$, becomes
\beq{second-derivative-function}
C_\vs \equiv \frac{\partial^2 V}{\partial \uvs^2} \approx \frac{V_\text{A} + V_\text{B} - 2V_0}{\Dsep^2},
\eeq
where $V_\text{x} \equiv V(\vR_\text{x})$.
As the gradient points away from each minimum, it is convenient to define a force, $\vF$ that points towards minima instead, for use in the iterative minima search,
\beq{gradient-force}
\vF_\text{x} \equiv - \nabla V(\vR_\text{x}).
\eeq
Should the gradient is readily available, as it often is, \fref{eq:second-derivative-function} can be rewritten to depend on it instead,
\beq{second-derviative-gradient}
C_\vs \approx \frac{(\vF_\text{B} - \vF_\text{A}) \cdot \uvs}{\Dsep}.
\eeq
Rotating $\uvs$ around $\vR_0$, according to the rotational force,
\beq{rotational-force}
%\vF^\circlearrowright = (\vF_\text{A} - (\vF_\text{A} \cdot \uvs)\uvs) - (\vF_\text{B} - (\vF_\text{B} \cdot \uvs)\uvs)
\vF^\circlearrowright = \vF_\text{A} - \vF_\text{B} + ((\vF_\text{B} - \vF_\text{A}) \cdot \uvs)\uvs,
\eeq
until $C_\vs$ is minimized yields an estimate for, both, the lowest eigenvalue, $C_\text{min} = C_\vs$, of the Hessian and its corresponding eigenmode, the minimum mode, $\uvn$.
The rotation happens in a plane spanned by $\uvn$ and $\uvT \equiv \vFÌ£^\circlearrowright / \left| \vF^\circlearrowright \right|$
A number of rotational schemes can be employed, such as a finite difference, conjugate gradient, approximation~\cite{dimer-original-1999} and, as described in~\cite{dimer-heyden-2005}, by expanding the curvature, exactly, as a Fourier series.
Both of the mentioned schemes require extra calculations to figure out the optimal angle of rotation but the latter is better suited when the accuracy and/or consistency of the force cannot be guaranteed~\cite{dimer-heyden-2005}.

Often $\vF_0$ is calculated to get a more accurate translational force, this can be taken advantage of in order to cut down the amount of computations.
Assuming a quadratic behaviour near the dimer, the gradient at either of the dimer's endpoints can be extrapolated from the other endpoint and the central point~\cite{dimer-olsen-2004},
\beq{dimer-point-extrapolate}
\vF_\text{B} = 2\vF_0 - \vF_\text{A},
\eeq
with $\vF_\text{B}$ being the extrapolated, virtual, force.
Since $\vF_0$ is static and does not require additional calculations during the iterative rotation, performing this extrapolation yields significant calculational reductions, up to a factor of half.

Further extrapolations are possible, if multiple rotations are performed (which is often not the case), it is possible to use previous calculations of either end point to extrapolate the rotated values.
These, however, yield much less reductions than the extrapolation in \fref{eq:dimer-point-extrapolate}.

\recent

\subsubsection{Gradient Transformation}
Once a minimum mode estimate is available for the current point, $\vR_0$, it is possible to transform the force so that any \sap1 is transformed to a minimum.
As discussed above, \sap1s are stationary (with zero gradient) and the Hessian has one and only one negative eigenvalue.
%As discussed above, saddle points are stationary and the Hessian has as many negative eigenvalues as the order of the saddle point, i.c. for a \sap1 the Hessian has one and only one negative eigenvalue.
The goal is thus to maximize the function's value along the minimum mode while minimizing it along all other eigenmodes.
This can be achieved, simply, by inverting any force components along the minimum mode,
\beq{dimer-transform}
\vF_0^\text{t} = \vF_0 - 2(\vF_0 \cdot \uvn)\uvn.
\eeq
In cases were the dimer aligns itself with a contour of the potential in a convex region (where the Hessian has only positive eigenvalues), it is possible that a lot of time will be spent there.
In order to circumvent this, a different force transformation,
\beq{dimer-transform-minima}
\vF_0^\text{t} = -(\vF_0 \cdot \uvn)\uvn ,
\eeq
is often used in convex regions.
This latter transformation simply inverts the force along the minimum mode while ignoring any other components.
This along with a fixed, artificially large, step size should yield less iterations spent near minima and more near \sap1s.~\cite{dimer-original-1999}

\recent

\subsubsection{Iterative Translation}
After the force has been transformed such that \sap1s appear as minima - \sap1s, however, remain unchanged with regards to the function's value - it is possible to use conventional algorithms for finding minima as long as they support a systematic increase in the function's value.
A finitie difference method was suggested in the original implementation~\cite{dimer-original-1999} while more recent papers~\cite{dimer-kastner-2008} have used other methods, such as the L-BFGS algorithm~\cite{lbfgs}.

\recent

